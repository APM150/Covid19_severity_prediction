{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "updating to the latest version\n",
      "update successful\n",
      "current working dir: d:\\dataset\\CS184A-Covid19_severity_prediction\n"
     ]
    }
   ],
   "source": [
    "# Download data, execute this block once in a while\n",
    "# checks if dataset is downloaded at data/covid19-severity-prediction\n",
    "# updating to the latest version\n",
    "# Note: this can take a while if you don't have the dataset in data/covid19-severity-prediction\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "current device: GeForce RTX 3080\n",
      "loading county-level data...\n",
      "loaded and merged COVID-19 cases/deaths data successfully\n"
     ]
    }
   ],
   "source": [
    "import load_data\n",
    "from Project_Models import RNN_Model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "print(\"current device:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\" normalize data of each column \"\"\"\n",
    "    maxtensor = x.max(0, keepdim=True)[0]\n",
    "    maxtensor[maxtensor==0] = 1e-4\n",
    "    x_normed = x / maxtensor\n",
    "    return x_normed, maxtensor\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "df = load_data.load_county_level('./data/covid19-severity-prediction/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "#x nan: 0\nx: tensor([[[0.1579, 0.1496, 0.2425, 0.7638, 0.0000, 0.0000],\n         [0.1579, 0.1496, 0.2425, 0.7638, 0.0000, 0.0000],\n         [0.1579, 0.1496, 0.2425, 0.7638, 0.0000, 0.0000],\n         ...,\n         [0.1579, 0.1496, 0.2425, 0.7638, 0.2434, 1.0000],\n         [0.1579, 0.1496, 0.2425, 0.7638, 0.2433, 1.0000],\n         [0.1579, 0.1496, 0.2425, 0.7638, 0.2426, 1.0000]],\n\n        [[0.0789, 0.0607, 0.2413, 0.8031, 0.0000, 0.0000],\n         [0.0789, 0.0607, 0.2413, 0.8031, 0.0000, 0.0000],\n         [0.0789, 0.0607, 0.2413, 0.8031, 0.0000, 0.0000],\n         ...,\n         [0.0789, 0.0607, 0.2413, 0.8031, 0.2464, 0.9824],\n         [0.0789, 0.0607, 0.2413, 0.8031, 0.2463, 0.9827],\n         [0.0789, 0.0607, 0.2413, 0.8031, 0.2456, 0.9825]],\n\n        [[1.0000, 1.0000, 1.0000, 0.6378, 0.0000, 0.0000],\n         [1.0000, 1.0000, 1.0000, 0.6378, 0.0000, 0.0000],\n         [1.0000, 1.0000, 1.0000, 0.6378, 0.0000, 0.0000],\n         ...,\n         [1.0000, 1.0000, 1.0000, 0.6378, 1.0000, 0.9648],\n         [1.0000, 1.0000, 1.0000, 0.6378, 1.0000, 0.9652],\n         [1.0000, 1.0000, 1.0000, 0.6378, 1.0000, 0.9677]],\n\n        ...,\n\n        [[0.1053, 0.2215, 0.0788, 0.9213, 0.0000, 0.0000],\n         [0.1053, 0.2215, 0.0788, 0.9213, 0.0000, 0.0000],\n         [0.1053, 0.2215, 0.0788, 0.9213, 0.0000, 0.0000],\n         ...,\n         [0.1053, 0.2215, 0.0788, 0.9213, 0.0802, 0.0542],\n         [0.1053, 0.2215, 0.0788, 0.9213, 0.0803, 0.0542],\n         [0.1053, 0.2215, 0.0788, 0.9213, 0.0805, 0.0550]],\n\n        [[0.1184, 0.1472, 0.1860, 0.5748, 0.0000, 0.0000],\n         [0.1184, 0.1472, 0.1860, 0.5748, 0.0000, 0.0000],\n         [0.1184, 0.1472, 0.1860, 0.5748, 0.0000, 0.0000],\n         ...,\n         [0.1184, 0.1472, 0.1860, 0.5748, 0.0822, 0.0578],\n         [0.1184, 0.1472, 0.1860, 0.5748, 0.0826, 0.0582],\n         [0.1184, 0.1472, 0.1860, 0.5748, 0.0828, 0.0586]],\n\n        [[0.0395, 0.0334, 0.0543, 0.8819, 0.0000, 0.0000],\n         [0.0395, 0.0334, 0.0543, 0.8819, 0.0000, 0.0000],\n         [0.0395, 0.0334, 0.0543, 0.8819, 0.0000, 0.0000],\n         ...,\n         [0.0395, 0.0334, 0.0543, 0.8819, 0.0321, 0.0588],\n         [0.0395, 0.0334, 0.0543, 0.8819, 0.0323, 0.0588],\n         [0.0395, 0.0334, 0.0543, 0.8819, 0.0325, 0.0587]]], device='cuda:0')\n#y nan: 0\ny: tensor([[ 79558.0000,   7441.0000],\n        [ 80555.0000,   7317.0000],\n        [327964.0000,   7216.0000],\n        [231462.0000,   5717.0000],\n        [ 57657.0000,   5014.0000],\n        [170207.0000,   3747.0002],\n        [195991.0000,   3693.0000],\n        [ 38668.0000,   3200.0000],\n        [ 48200.0000,   3104.0000],\n        [168746.0156,   2877.0000],\n        [ 38280.0000,   2333.0000],\n        [ 52575.0000,   2226.0000],\n        [ 28451.0000,   2161.0000],\n        [ 27678.0000,   2075.0000],\n        [ 51744.0039,   2023.0001],\n        [ 45578.0000,   1914.0000],\n        [ 37031.0000,   1750.0000],\n        [ 56509.0039,   1614.0000],\n        [ 90885.0000,   1590.0000],\n        [ 92148.0000,   1564.0000],\n        [ 25671.0000,   1540.0000],\n        [ 21211.9980,   1525.0000],\n        [ 63165.0000,   1514.0000],\n        [ 42940.0000,   1482.0000],\n        [ 24606.0000,   1460.0000],\n        [ 28731.0000,   1451.0000],\n        [ 68376.0000,   1438.0000],\n        [ 22824.0000,   1383.0000],\n        [ 27938.0000,   1382.0000],\n        [ 72341.0000,   1352.0000],\n        [104450.9922,   1341.0000],\n        [ 23388.0000,   1273.0000],\n        [ 32405.9980,   1260.0000],\n        [ 21656.0000,   1204.0000],\n        [ 32542.0000,   1201.0000],\n        [ 20892.0000,   1151.0000],\n        [ 14642.0010,   1136.0000],\n        [ 24953.0000,   1130.0000],\n        [ 18876.0000,   1104.0000],\n        [ 71072.0000,   1096.0000],\n        [ 17933.0000,   1080.0000],\n        [ 46555.0000,   1033.0000],\n        [ 24825.0000,    993.0000],\n        [ 27808.0020,    986.0000],\n        [ 61053.0000,    915.0000],\n        [ 16115.0000,    900.0000],\n        [ 35074.0000,    899.0000],\n        [ 27969.0000,    899.0000],\n        [ 68042.0000,    892.0000],\n        [ 35146.0000,    876.0000],\n        [ 15729.0000,    876.0000],\n        [ 32204.0000,    863.0000],\n        [ 12993.0010,    862.0000],\n        [ 51635.0000,    851.0000],\n        [ 10550.0000,    844.0000],\n        [ 12251.0010,    842.0000],\n        [ 28242.0000,    838.0000],\n        [ 31496.9980,    829.0000],\n        [ 15420.0000,    784.0000],\n        [ 15592.0000,    779.0000],\n        [ 67484.0000,    729.0000],\n        [ 26048.0000,    715.0000],\n        [ 23104.0020,    688.0000],\n        [ 16028.0010,    676.0000],\n        [ 31204.0020,    669.0000],\n        [ 18379.0000,    657.0000],\n        [ 32487.0000,    648.0000],\n        [ 39917.0000,    645.0000],\n        [ 33099.0000,    644.0000],\n        [ 13638.0000,    644.0000],\n        [ 10368.0000,    643.0000],\n        [ 24661.0020,    643.0000],\n        [ 11921.0000,    635.0000],\n        [ 55471.0000,    632.0000],\n        [ 25791.0000,    609.0000],\n        [ 41003.0000,    603.0000],\n        [ 14034.0000,    602.0000],\n        [ 49752.0000,    601.0000],\n        [  7212.0000,    600.0000],\n        [ 19302.0000,    592.0000],\n        [ 37494.0000,    544.0000],\n        [ 25316.0000,    538.0000],\n        [ 27920.0000,    538.0000],\n        [  9728.0000,    522.0000],\n        [ 19768.0000,    519.0000],\n        [ 19277.0000,    516.0000],\n        [ 29014.0000,    514.0000],\n        [ 22993.0000,    500.0000],\n        [ 11271.0000,    492.0000],\n        [ 25249.0000,    483.0000],\n        [ 23400.9980,    479.0000],\n        [ 26627.0000,    470.0000],\n        [ 32648.0000,    465.0000],\n        [ 16793.0000,    462.0000],\n        [ 24769.0000,    461.0000],\n        [ 32755.0000,    456.0000],\n        [ 33559.0000,    456.0000],\n        [ 26514.0000,    442.0000],\n        [ 27124.0000,    440.0000],\n        [ 10766.9990,    438.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# Preprocess Data\n",
    "\n",
    "maxload = 100\n",
    "\n",
    "x = form_input_tensor(df, ['#Hospitals', '#ICU_beds', 'MedicareEnrollment,AgedTot2017', 'DiabetesPercentage'], maxload=maxload).to(device)\n",
    "x, xmaxtensor = normalize(x)\n",
    "print(f\"#x nan: {(torch.sum(torch.isnan(x)))}\")\n",
    "print(\"x:\", x)\n",
    "\n",
    "y = form_labels_tensor(df, maxload=maxload).to(device)\n",
    "y, ymaxtensor = normalize(y)\n",
    "print(f\"#y nan: {torch.sum(torch.isnan(y))}\")\n",
    "print(\"y:\", y * ymaxtensor)\n",
    "\n",
    "Xtrain = x[:90]\n",
    "Ytrain = y[:90]\n",
    "Xtest  = x[90:]\n",
    "Ytest  = y[90:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Before training: tensor([[-1.5259e+04,  6.6340e+01],\n",
      "        [-1.5331e+04,  4.9460e+01],\n",
      "        [-1.2547e+04,  2.6528e+02],\n",
      "        [-1.2354e+04,  1.3112e+02],\n",
      "        [-1.4370e+04, -4.1639e+01],\n",
      "        [-1.0369e+04,  1.2539e+02],\n",
      "        [-8.5304e+03,  1.3066e+02],\n",
      "        [-1.0967e+04,  1.2689e+02],\n",
      "        [-1.2275e+04,  1.9727e+01],\n",
      "        [-9.9505e+03,  4.2874e+01],\n",
      "        [-9.2117e+03,  1.0870e+02],\n",
      "        [-9.6257e+03,  9.6876e+01],\n",
      "        [-1.0686e+04,  2.2824e+00],\n",
      "        [-9.0206e+03,  9.6022e+01],\n",
      "        [-8.9036e+03,  1.0759e+02],\n",
      "        [-1.0854e+04,  2.8763e+00],\n",
      "        [-1.0857e+04, -4.1959e+01],\n",
      "        [-9.2015e+03,  8.7703e+01],\n",
      "        [-9.6140e+03,  2.2724e+01],\n",
      "        [-8.8863e+03,  7.1919e+01],\n",
      "        [-8.8522e+03,  4.1370e+01],\n",
      "        [-9.5966e+03,  4.2470e+01],\n",
      "        [-9.4445e+03,  1.1170e+02],\n",
      "        [-7.8109e+03,  1.0344e+02],\n",
      "        [-9.9893e+03,  6.3579e+00],\n",
      "        [-8.1714e+03,  1.0065e+02],\n",
      "        [-9.4291e+03,  5.5570e+01],\n",
      "        [-8.5160e+03,  6.0609e+01],\n",
      "        [-8.9765e+03,  4.6527e+01],\n",
      "        [-8.9195e+03,  6.0743e+01],\n",
      "        [-8.0168e+03,  6.1081e+01],\n",
      "        [-8.6403e+03,  5.0756e+01],\n",
      "        [-8.9873e+03,  7.4784e+01],\n",
      "        [-8.6743e+03,  5.6911e+01],\n",
      "        [-9.3964e+03,  1.8118e+01],\n",
      "        [-9.1037e+03,  3.9375e+01],\n",
      "        [-8.3668e+03,  7.1570e+01],\n",
      "        [-9.5950e+03,  1.2907e+01],\n",
      "        [-9.5609e+03,  5.1287e-01],\n",
      "        [-8.8254e+03,  3.0046e+01],\n",
      "        [-8.9704e+03,  4.6586e+01],\n",
      "        [-7.7286e+03,  1.0185e+02],\n",
      "        [-9.3420e+03, -1.4804e+01],\n",
      "        [-9.2029e+03,  1.4457e+01],\n",
      "        [-8.8128e+03,  1.3286e+02],\n",
      "        [-7.9943e+03,  8.4857e+01],\n",
      "        [-9.8387e+03,  1.2709e+01],\n",
      "        [-7.9171e+03,  8.3092e+01],\n",
      "        [-8.8506e+03,  2.2562e+00],\n",
      "        [-1.0268e+04, -6.5651e+01],\n",
      "        [-8.3144e+03,  6.4291e+01],\n",
      "        [-1.0715e+04, -4.8876e+01],\n",
      "        [-8.9860e+03,  1.9399e+01],\n",
      "        [-9.2282e+03,  2.6471e+01],\n",
      "        [-7.6227e+03,  9.3679e+01],\n",
      "        [-9.2155e+03,  4.2611e+00],\n",
      "        [-8.6666e+03,  6.5181e+01],\n",
      "        [-8.3277e+03,  1.0270e+02],\n",
      "        [-9.0250e+03,  1.3700e+01],\n",
      "        [-8.8497e+03,  2.1794e+01],\n",
      "        [-8.1632e+03,  1.6397e+00],\n",
      "        [-9.8399e+03,  3.1750e+01],\n",
      "        [-9.0499e+03,  1.3608e+01],\n",
      "        [-9.6223e+03,  1.3281e+01],\n",
      "        [-8.0497e+03,  8.3348e+01],\n",
      "        [-8.8962e+03,  2.2838e+01],\n",
      "        [-8.1796e+03,  6.0290e+01],\n",
      "        [-9.1811e+03,  1.2308e+01],\n",
      "        [-8.8663e+03,  4.1598e+01],\n",
      "        [-9.8539e+03, -3.0490e+01],\n",
      "        [-8.2461e+03,  3.8799e+01],\n",
      "        [-1.0672e+04, -7.7299e+01],\n",
      "        [-7.3564e+03,  9.5870e+01],\n",
      "        [-9.1072e+03, -1.0860e+01],\n",
      "        [-7.2364e+03,  1.1121e+02],\n",
      "        [-9.8793e+03, -4.6713e+01],\n",
      "        [-9.2426e+03, -2.4009e+01],\n",
      "        [-9.1177e+03,  1.5260e+01],\n",
      "        [-7.2575e+03,  9.0403e+01],\n",
      "        [-1.0694e+04, -8.7275e+01],\n",
      "        [-1.0096e+04, -4.0985e+01],\n",
      "        [-8.3996e+03,  5.3491e+01],\n",
      "        [-7.7393e+03,  5.0769e+01],\n",
      "        [-9.2059e+03, -9.2482e+00],\n",
      "        [-1.0573e+04, -7.0313e+01],\n",
      "        [-7.2295e+03,  6.9777e+01],\n",
      "        [-9.1736e+03,  3.7625e+01],\n",
      "        [-8.8967e+03, -1.0508e+01],\n",
      "        [-8.2822e+03,  4.0601e+01],\n",
      "        [-7.5757e+03,  1.0116e+02]], device='cuda:0')\n",
      "Epoch [1/300], Loss: 0.0611\n",
      "Epoch [2/300], Loss: 0.0273\n",
      "Epoch [3/300], Loss: 0.0343\n",
      "Epoch [4/300], Loss: 0.0308\n",
      "Epoch [5/300], Loss: 0.0231\n",
      "Epoch [6/300], Loss: 0.0199\n",
      "Epoch [7/300], Loss: 0.0202\n",
      "Epoch [8/300], Loss: 0.0200\n",
      "Epoch [9/300], Loss: 0.0173\n",
      "Epoch [10/300], Loss: 0.0132\n",
      "Epoch [11/300], Loss: 0.0101\n",
      "Epoch [12/300], Loss: 0.0093\n",
      "Epoch [13/300], Loss: 0.0086\n",
      "Epoch [14/300], Loss: 0.0063\n",
      "Epoch [15/300], Loss: 0.0060\n",
      "Epoch [16/300], Loss: 0.0076\n",
      "Epoch [17/300], Loss: 0.0063\n",
      "Epoch [18/300], Loss: 0.0048\n",
      "Epoch [19/300], Loss: 0.0050\n",
      "Epoch [20/300], Loss: 0.0047\n",
      "Epoch [21/300], Loss: 0.0037\n",
      "Epoch [22/300], Loss: 0.0030\n",
      "Epoch [23/300], Loss: 0.0029\n",
      "Epoch [24/300], Loss: 0.0030\n",
      "Epoch [25/300], Loss: 0.0029\n",
      "Epoch [26/300], Loss: 0.0024\n",
      "Epoch [27/300], Loss: 0.0017\n",
      "Epoch [28/300], Loss: 0.0012\n",
      "Epoch [29/300], Loss: 0.0012\n",
      "Epoch [30/300], Loss: 0.0012\n",
      "Epoch [31/300], Loss: 0.0012\n",
      "Epoch [32/300], Loss: 0.0013\n",
      "Epoch [33/300], Loss: 0.0013\n",
      "Epoch [34/300], Loss: 0.0014\n",
      "Epoch [35/300], Loss: 0.0013\n",
      "Epoch [36/300], Loss: 0.0012\n",
      "Epoch [37/300], Loss: 0.0009\n",
      "Epoch [38/300], Loss: 0.0007\n",
      "Epoch [39/300], Loss: 0.0007\n",
      "Epoch [40/300], Loss: 0.0007\n",
      "Epoch [41/300], Loss: 0.0006\n",
      "Epoch [42/300], Loss: 0.0006\n",
      "Epoch [43/300], Loss: 0.0006\n",
      "Epoch [44/300], Loss: 0.0007\n",
      "Epoch [45/300], Loss: 0.0007\n",
      "Epoch [46/300], Loss: 0.0006\n",
      "Epoch [47/300], Loss: 0.0005\n",
      "Epoch [48/300], Loss: 0.0005\n",
      "Epoch [49/300], Loss: 0.0005\n",
      "Epoch [50/300], Loss: 0.0005\n",
      "Epoch [51/300], Loss: 0.0005\n",
      "Epoch [52/300], Loss: 0.0004\n",
      "Epoch [53/300], Loss: 0.0004\n",
      "Epoch [54/300], Loss: 0.0004\n",
      "Epoch [55/300], Loss: 0.0004\n",
      "Epoch [56/300], Loss: 0.0003\n",
      "Epoch [57/300], Loss: 0.0003\n",
      "Epoch [58/300], Loss: 0.0003\n",
      "Epoch [59/300], Loss: 0.0003\n",
      "Epoch [60/300], Loss: 0.0003\n",
      "Epoch [61/300], Loss: 0.0003\n",
      "Epoch [62/300], Loss: 0.0003\n",
      "Epoch [63/300], Loss: 0.0002\n",
      "Epoch [64/300], Loss: 0.0002\n",
      "Epoch [65/300], Loss: 0.0002\n",
      "Epoch [66/300], Loss: 0.0002\n",
      "Epoch [67/300], Loss: 0.0002\n",
      "Epoch [68/300], Loss: 0.0002\n",
      "Epoch [69/300], Loss: 0.0002\n",
      "Epoch [70/300], Loss: 0.0002\n",
      "Epoch [71/300], Loss: 0.0002\n",
      "Epoch [72/300], Loss: 0.0002\n",
      "Epoch [73/300], Loss: 0.0001\n",
      "Epoch [74/300], Loss: 0.0001\n",
      "Epoch [75/300], Loss: 0.0001\n",
      "Epoch [76/300], Loss: 0.0001\n",
      "Epoch [77/300], Loss: 0.0001\n",
      "Epoch [78/300], Loss: 0.0001\n",
      "Epoch [79/300], Loss: 0.0001\n",
      "Epoch [80/300], Loss: 0.0001\n",
      "Epoch [81/300], Loss: 0.0001\n",
      "Epoch [82/300], Loss: 0.0001\n",
      "Epoch [83/300], Loss: 0.0001\n",
      "Epoch [84/300], Loss: 0.0001\n",
      "Epoch [85/300], Loss: 0.0001\n",
      "Epoch [86/300], Loss: 0.0001\n",
      "Epoch [87/300], Loss: 0.0001\n",
      "Epoch [88/300], Loss: 0.0001\n",
      "Epoch [89/300], Loss: 0.0001\n",
      "Epoch [90/300], Loss: 0.0001\n",
      "Epoch [91/300], Loss: 0.0001\n",
      "Epoch [92/300], Loss: 0.0001\n",
      "Epoch [93/300], Loss: 0.0001\n",
      "Epoch [94/300], Loss: 0.0001\n",
      "Epoch [95/300], Loss: 0.0001\n",
      "Epoch [96/300], Loss: 0.0000\n",
      "Epoch [97/300], Loss: 0.0000\n",
      "Epoch [98/300], Loss: 0.0000\n",
      "Epoch [99/300], Loss: 0.0000\n",
      "Epoch [100/300], Loss: 0.0000\n",
      "Epoch [101/300], Loss: 0.0000\n",
      "Epoch [102/300], Loss: 0.0000\n",
      "Epoch [103/300], Loss: 0.0000\n",
      "Epoch [104/300], Loss: 0.0000\n",
      "Epoch [105/300], Loss: 0.0000\n",
      "Epoch [106/300], Loss: 0.0000\n",
      "Epoch [107/300], Loss: 0.0000\n",
      "Epoch [108/300], Loss: 0.0000\n",
      "Epoch [109/300], Loss: 0.0000\n",
      "Epoch [110/300], Loss: 0.0000\n",
      "Epoch [111/300], Loss: 0.0000\n",
      "Epoch [112/300], Loss: 0.0000\n",
      "Epoch [113/300], Loss: 0.0000\n",
      "Epoch [114/300], Loss: 0.0000\n",
      "Epoch [115/300], Loss: 0.0000\n",
      "Epoch [116/300], Loss: 0.0000\n",
      "Epoch [117/300], Loss: 0.0000\n",
      "Epoch [118/300], Loss: 0.0000\n",
      "Epoch [119/300], Loss: 0.0000\n",
      "Epoch [120/300], Loss: 0.0000\n",
      "Epoch [121/300], Loss: 0.0000\n",
      "Epoch [122/300], Loss: 0.0000\n",
      "Epoch [123/300], Loss: 0.0000\n",
      "Epoch [124/300], Loss: 0.0000\n",
      "Epoch [125/300], Loss: 0.0000\n",
      "Epoch [126/300], Loss: 0.0000\n",
      "Epoch [127/300], Loss: 0.0000\n",
      "Epoch [128/300], Loss: 0.0000\n",
      "Epoch [129/300], Loss: 0.0000\n",
      "Epoch [130/300], Loss: 0.0000\n",
      "Epoch [131/300], Loss: 0.0000\n",
      "Epoch [132/300], Loss: 0.0000\n",
      "Epoch [133/300], Loss: 0.0000\n",
      "Epoch [134/300], Loss: 0.0000\n",
      "Epoch [135/300], Loss: 0.0000\n",
      "Epoch [136/300], Loss: 0.0000\n",
      "Epoch [137/300], Loss: 0.0000\n",
      "Epoch [138/300], Loss: 0.0000\n",
      "Epoch [139/300], Loss: 0.0000\n",
      "Epoch [140/300], Loss: 0.0000\n",
      "Epoch [141/300], Loss: 0.0000\n",
      "Epoch [142/300], Loss: 0.0000\n",
      "Epoch [143/300], Loss: 0.0000\n",
      "Epoch [144/300], Loss: 0.0000\n",
      "Epoch [145/300], Loss: 0.0000\n",
      "Epoch [146/300], Loss: 0.0000\n",
      "Epoch [147/300], Loss: 0.0000\n",
      "Epoch [148/300], Loss: 0.0000\n",
      "Epoch [149/300], Loss: 0.0000\n",
      "Epoch [150/300], Loss: 0.0000\n",
      "Epoch [151/300], Loss: 0.0000\n",
      "Epoch [152/300], Loss: 0.0000\n",
      "Epoch [153/300], Loss: 0.0000\n",
      "Epoch [154/300], Loss: 0.0000\n",
      "Epoch [155/300], Loss: 0.0000\n",
      "Epoch [156/300], Loss: 0.0000\n",
      "Epoch [157/300], Loss: 0.0000\n",
      "Epoch [158/300], Loss: 0.0000\n",
      "Epoch [159/300], Loss: 0.0000\n",
      "Epoch [160/300], Loss: 0.0000\n",
      "Epoch [161/300], Loss: 0.0000\n",
      "Epoch [162/300], Loss: 0.0000\n",
      "Epoch [163/300], Loss: 0.0000\n",
      "Epoch [164/300], Loss: 0.0000\n",
      "Epoch [165/300], Loss: 0.0000\n",
      "Epoch [166/300], Loss: 0.0000\n",
      "Epoch [167/300], Loss: 0.0000\n",
      "Epoch [168/300], Loss: 0.0000\n",
      "Epoch [169/300], Loss: 0.0000\n",
      "Epoch [170/300], Loss: 0.0000\n",
      "Epoch [171/300], Loss: 0.0000\n",
      "Epoch [172/300], Loss: 0.0000\n",
      "Epoch [173/300], Loss: 0.0000\n",
      "Epoch [174/300], Loss: 0.0000\n",
      "Epoch [175/300], Loss: 0.0000\n",
      "Epoch [176/300], Loss: 0.0000\n",
      "Epoch [177/300], Loss: 0.0000\n",
      "Epoch [178/300], Loss: 0.0000\n",
      "Epoch [179/300], Loss: 0.0000\n",
      "Epoch [180/300], Loss: 0.0000\n",
      "Epoch [181/300], Loss: 0.0000\n",
      "Epoch [182/300], Loss: 0.0000\n",
      "Epoch [183/300], Loss: 0.0000\n",
      "Epoch [184/300], Loss: 0.0000\n",
      "Epoch [185/300], Loss: 0.0000\n",
      "Epoch [186/300], Loss: 0.0000\n",
      "Epoch [187/300], Loss: 0.0000\n",
      "Epoch [188/300], Loss: 0.0000\n",
      "Epoch [189/300], Loss: 0.0000\n",
      "Epoch [190/300], Loss: 0.0000\n",
      "Epoch [191/300], Loss: 0.0000\n",
      "Epoch [192/300], Loss: 0.0000\n",
      "Epoch [193/300], Loss: 0.0000\n",
      "Epoch [194/300], Loss: 0.0000\n",
      "Epoch [195/300], Loss: 0.0000\n",
      "Epoch [196/300], Loss: 0.0000\n",
      "Epoch [197/300], Loss: 0.0000\n",
      "Epoch [198/300], Loss: 0.0000\n",
      "Epoch [199/300], Loss: 0.0000\n",
      "Epoch [200/300], Loss: 0.0000\n",
      "Epoch [201/300], Loss: 0.0000\n",
      "Epoch [202/300], Loss: 0.0000\n",
      "Epoch [203/300], Loss: 0.0000\n",
      "Epoch [204/300], Loss: 0.0000\n",
      "Epoch [205/300], Loss: 0.0000\n",
      "Epoch [206/300], Loss: 0.0000\n",
      "Epoch [207/300], Loss: 0.0000\n",
      "Epoch [208/300], Loss: 0.0000\n",
      "Epoch [209/300], Loss: 0.0000\n",
      "Epoch [210/300], Loss: 0.0000\n",
      "Epoch [211/300], Loss: 0.0000\n",
      "Epoch [212/300], Loss: 0.0000\n",
      "Epoch [213/300], Loss: 0.0000\n",
      "Epoch [214/300], Loss: 0.0000\n",
      "Epoch [215/300], Loss: 0.0000\n",
      "Epoch [216/300], Loss: 0.0000\n",
      "Epoch [217/300], Loss: 0.0000\n",
      "Epoch [218/300], Loss: 0.0000\n",
      "Epoch [219/300], Loss: 0.0000\n",
      "Epoch [220/300], Loss: 0.0000\n",
      "Epoch [221/300], Loss: 0.0000\n",
      "Epoch [222/300], Loss: 0.0000\n",
      "Epoch [223/300], Loss: 0.0000\n",
      "Epoch [224/300], Loss: 0.0000\n",
      "Epoch [225/300], Loss: 0.0000\n",
      "Epoch [226/300], Loss: 0.0000\n",
      "Epoch [227/300], Loss: 0.0000\n",
      "Epoch [228/300], Loss: 0.0000\n",
      "Epoch [229/300], Loss: 0.0000\n",
      "Epoch [230/300], Loss: 0.0000\n",
      "Epoch [231/300], Loss: 0.0000\n",
      "Epoch [232/300], Loss: 0.0000\n",
      "Epoch [233/300], Loss: 0.0000\n",
      "Epoch [234/300], Loss: 0.0000\n",
      "Epoch [235/300], Loss: 0.0000\n",
      "Epoch [236/300], Loss: 0.0000\n",
      "Epoch [237/300], Loss: 0.0000\n",
      "Epoch [238/300], Loss: 0.0000\n",
      "Epoch [239/300], Loss: 0.0000\n",
      "Epoch [240/300], Loss: 0.0000\n",
      "Epoch [241/300], Loss: 0.0000\n",
      "Epoch [242/300], Loss: 0.0000\n",
      "Epoch [243/300], Loss: 0.0000\n",
      "Epoch [244/300], Loss: 0.0000\n",
      "Epoch [245/300], Loss: 0.0000\n",
      "Epoch [246/300], Loss: 0.0000\n",
      "Epoch [247/300], Loss: 0.0000\n",
      "Epoch [248/300], Loss: 0.0000\n",
      "Epoch [249/300], Loss: 0.0000\n",
      "Epoch [250/300], Loss: 0.0000\n",
      "Epoch [251/300], Loss: 0.0000\n",
      "Epoch [252/300], Loss: 0.0000\n",
      "Epoch [253/300], Loss: 0.0000\n",
      "Epoch [254/300], Loss: 0.0000\n",
      "Epoch [255/300], Loss: 0.0000\n",
      "Epoch [256/300], Loss: 0.0000\n",
      "Epoch [257/300], Loss: 0.0000\n",
      "Epoch [258/300], Loss: 0.0000\n",
      "Epoch [259/300], Loss: 0.0000\n",
      "Epoch [260/300], Loss: 0.0000\n",
      "Epoch [261/300], Loss: 0.0000\n",
      "Epoch [262/300], Loss: 0.0000\n",
      "Epoch [263/300], Loss: 0.0000\n",
      "Epoch [264/300], Loss: 0.0000\n",
      "Epoch [265/300], Loss: 0.0000\n",
      "Epoch [266/300], Loss: 0.0000\n",
      "Epoch [267/300], Loss: 0.0000\n",
      "Epoch [268/300], Loss: 0.0000\n",
      "Epoch [269/300], Loss: 0.0000\n",
      "Epoch [270/300], Loss: 0.0000\n",
      "Epoch [271/300], Loss: 0.0000\n",
      "Epoch [272/300], Loss: 0.0000\n",
      "Epoch [273/300], Loss: 0.0000\n",
      "Epoch [274/300], Loss: 0.0000\n",
      "Epoch [275/300], Loss: 0.0000\n",
      "Epoch [276/300], Loss: 0.0000\n",
      "Epoch [277/300], Loss: 0.0000\n",
      "Epoch [278/300], Loss: 0.0000\n",
      "Epoch [279/300], Loss: 0.0000\n",
      "Epoch [280/300], Loss: 0.0000\n",
      "Epoch [281/300], Loss: 0.0000\n",
      "Epoch [282/300], Loss: 0.0000\n",
      "Epoch [283/300], Loss: 0.0000\n",
      "Epoch [284/300], Loss: 0.0000\n",
      "Epoch [285/300], Loss: 0.0000\n",
      "Epoch [286/300], Loss: 0.0000\n",
      "Epoch [287/300], Loss: 0.0000\n",
      "Epoch [288/300], Loss: 0.0000\n",
      "Epoch [289/300], Loss: 0.0000\n",
      "Epoch [290/300], Loss: 0.0000\n",
      "Epoch [291/300], Loss: 0.0000\n",
      "Epoch [292/300], Loss: 0.0000\n",
      "Epoch [293/300], Loss: 0.0000\n",
      "Epoch [294/300], Loss: 0.0000\n",
      "Epoch [295/300], Loss: 0.0000\n",
      "Epoch [296/300], Loss: 0.0000\n",
      "Epoch [297/300], Loss: 0.0000\n",
      "Epoch [298/300], Loss: 0.0000\n",
      "Epoch [299/300], Loss: 0.0000\n",
      "Epoch [300/300], Loss: 0.0000\n",
      "After training: tensor([[23312.0234,   453.0132],\n",
      "        [25206.8809,   453.1527],\n",
      "        [32484.2969,   453.1196],\n",
      "        [17141.7695,   454.8440],\n",
      "        [24409.1562,   431.7358],\n",
      "        [33058.2695,   449.4767],\n",
      "        [34448.5273,   431.7010],\n",
      "        [26960.3906,   390.1892],\n",
      "        [27452.2422,   428.8968],\n",
      "        [10284.9648,   434.1107]], device='cuda:0')\n",
      "y: tensor([[23400.9980,   479.0000],\n",
      "        [26627.0000,   470.0000],\n",
      "        [32648.0000,   465.0000],\n",
      "        [16793.0000,   462.0000],\n",
      "        [24769.0000,   461.0000],\n",
      "        [32755.0000,   456.0000],\n",
      "        [33559.0000,   456.0000],\n",
      "        [26514.0000,   442.0000],\n",
      "        [27124.0000,   440.0000],\n",
      "        [10766.9990,   438.0000]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "\n",
    "model = RNN_Model.RNN(Xtrain.shape[2], 128, 2).to(device)\n",
    "with torch.no_grad():\n",
    "    print(\"Before training:\", model(Xtrain) * ymaxtensor)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.SmoothL1Loss()\n",
    "learning_rate = 1e-3\n",
    "num_epoches = 300\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    outputs = model(Xtrain)\n",
    "\n",
    "    loss = criterion(outputs, Ytrain)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    print(f'Epoch [{epoch + 1}/{num_epoches}], Loss: {loss.item():.4f}')\n",
    "with torch.no_grad():\n",
    "    print(\"After training:\", (model(Xtest) * ymaxtensor))\n",
    "    print(\"y:\", (Ytest * ymaxtensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "# torch.save(model, \"saved_model/first100dataloss=0.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO\n",
    "# 1. Batch Gradient Descent using dataloader\n",
    "# 2. Handle NaN case"
   ]
  }
 ]
}